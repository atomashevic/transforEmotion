% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/video_scores.R
\name{video_scores}
\alias{video_scores}
\title{Run FER on a YouTube video using a Hugging Face CLIP model}
\usage{
video_scores(
  video,
  classes,
  nframes = 100,
  face_selection = "largest",
  start = 0,
  end = -1,
  uniform = FALSE,
  ffreq = 15,
  save_video = FALSE,
  save_frames = FALSE,
  save_dir = "temp/",
  video_name = "temp",
  model = "oai-base"
)
}
\arguments{
\item{video}{The URL of the YouTube video to analyze.}

\item{classes}{A character vector specifying the classes to analyze.}

\item{nframes}{The number of frames to analyze in the video. Default is 100.}

\item{face_selection}{The method for selecting faces in the video. Options are "largest", "left", or "right". Default is "largest".}

\item{start}{The start time of the video range to analyze. Default is 0.}

\item{end}{The end time of the video range to analyze. Default is -1 and this means that video won't be cut. If end is a positive number greater than start, the video will be cut from start to end.}

\item{uniform}{Logical indicating whether to uniformly sample frames from the video. Default is FALSE.}

\item{ffreq}{The frame frequency for sampling frames from the video. Default is 15.}

\item{save_video}{Logical indicating whether to save the analyzed video. Default is FALSE.}

\item{save_frames}{Logical indicating whether to save the analyzed frames. Default is FALSE.}

\item{save_dir}{The directory to save the analyzed frames. Default is "temp/".}

\item{video_name}{The name of the analyzed video. Default is "temp".}

\item{model}{A string specifying the CLIP model to use. Options are:
\itemize{
  \item \code{"oai-base"}: "openai/clip-vit-base-patch32" (default)
  \item \code{"oai-large"}: "openai/clip-vit-large-patch14"
  \item \code{"eva-8B"}: "BAAI/EVA-CLIP-8B-448" (quantized version for reduced memory usage)
  \item \code{"jina-v2"}: "jinaai/jina-clip-v2"
}}
}
\value{
A result object containing the analyzed video scores.
}
\description{
This function retrieves facial expression recognition (FER) scores from a specific number of frames extracted from a YouTube video using a specified Hugging Face CLIP model. It utilizes Python libraries for facial recognition and emotion detection in text, images, and video.
}
\section{Data Privacy}{

  All processing is done locally with the downloaded model,
  and your video frames are never sent to any remote server or third-party.
}

\author{
Aleksandar Tomasevic <atomashevic@gmail.com>
}
