% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/transformer_scores.R
\name{transformer_scores}
\alias{transformer_scores}
\title{Sentiment Analysis Scores}
\usage{
transformer_scores(
  text,
  classes,
  multiple_classes = FALSE,
  transformer = c("cross-encoder-roberta", "cross-encoder-distilroberta",
    "facebook-bart"),
  preprocess = FALSE,
  keep_in_env = TRUE,
  envir = 1
)
}
\arguments{
\item{text}{Character vector or list.
Text in a vector or list data format}

\item{classes}{Character vector.
Classes to score the text}

\item{multiple_classes}{Boolean.
Whether the text can belong to multiple true classes.
Defaults to \code{FALSE}.
Set to \code{TRUE} to get scores with multiple classes}

\item{transformer}{Character.
Specific zero-shot sentiment analysis transformer
to be used. Default options:

\describe{

\item{\code{"cross-encoder-roberta"}}{Uses \href{https://huggingface.co/cross-encoder/nli-roberta-base}{Cross-Encoder's Natural Language Interface RoBERTa Base}
zero-shot classification model trained on the
\href{https://nlp.stanford.edu/projects/snli/}{Stanford Natural Language Inference}
(SNLI) corpus and 
\href{https://huggingface.co/datasets/multi_nli}{MultiNLI} datasets}

\item{\code{"cross-encoder-distilroberta"}}{Uses \href{https://huggingface.co/cross-encoder/nli-distilroberta-base}{Cross-Encoder's Natural Language Interface DistilRoBERTa Base}
zero-shot classification model trained on the
\href{https://nlp.stanford.edu/projects/snli/}{Stanford Natural Language Inference}
(SNLI) corpus and 
\href{https://huggingface.co/datasets/multi_nli}{MultiNLI} datasets. The DistilRoBERTa
is intended to be a smaller, more lightweight version of \code{"cross-encoder-roberta"},
that sacrifices some accuracy for much faster speed (see 
\href{https://www.sbert.net/docs/pretrained_cross-encoders.html#nli}{https://www.sbert.net/docs/pretrained_cross-encoders.html#nli})}

\item{\code{"facebook-bart"}}{Uses \href{https://huggingface.co/facebook/bart-large-mnli}{Facebook's BART Large}
zero-shot classification model trained on the
\href{https://huggingface.co/datasets/multi_nli}{Multi-Genre Natural Language
Inference} (MultiNLI) dataset}

}

Defaults to \code{"cross-encoder-distilroberta"}

Also allows any zero-shot classification models with a pipeline
from \href{https://huggingface.co/models?pipeline_tag=zero-shot-classification}{huggingface}
to be used by using the specified name (e.g., \code{"typeform/distilbert-base-uncased-mnli"}; see Examples)}

\item{preprocess}{Boolean.
Should basic preprocessing be applied?
Includes making lowercase, keeping only alphanumeric characters,
removing escape characters, removing repeated characters,
and removing white space.
Defaults to \code{FALSE}.
Transformers generally are OK without preprocessing and handle
many of these functions internally, so setting to \code{TRUE}
will not change performance much}

\item{keep_in_env}{Boolean.
Whether the classifier should be kept in your global environment.
Defaults to \code{TRUE}.
By keeping the classifier in your environment, you can skip
re-loading the classifier every time you run this function.
\code{TRUE} is recommended}

\item{envir}{Numeric.
Environment for the classifier to be saved for repeated use.
Defaults to the global environment}
}
\value{
Returns probabilities for the text classes
}
\description{
Uses sentiment analysis pipelines from \href{https://huggingface.co}{huggingface}
to compute probabilities that the text corresponds to the specified classes
}
\examples{
# Load data
data(neo_ipip_extraversion)

# Example text 
text <- neo_ipip_extraversion$friendliness[1:5]

\dontrun{
# Cross-Encoder DistilRoBERTa
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 )
)

# Facebook BART Large
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 transformer = "facebook-bart"
)

# Directly from huggingface: typeform/distilbert-base-uncased-mnli
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 transformer = "typeform/distilbert-base-uncased-mnli"
)
}

}
\references{
# BART \cr
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ... & Zettlemoyer, L. (2019).
Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.
\emph{arXiv preprint arXiv:1910.13461}.

# RoBERTa \cr
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019).
Roberta: A robustly optimized bert pretraining approach.
\emph{arXiv preprint arXiv:1907.11692}.

# Zero-shot classification \cr
Yin, W., Hay, J., & Roth, D. (2019).
Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach.
\emph{arXiv preprint arXiv:1909.00161}.

# MultiNLI dataset \cr
Williams, A., Nangia, N., & Bowman, S. R. (2017).
A broad-coverage challenge corpus for sentence understanding through inference.
\emph{arXiv preprint arXiv:1704.05426}.
}
\author{
Alexander P. Christensen <alexpaulchristensen@gmail.com>
}
