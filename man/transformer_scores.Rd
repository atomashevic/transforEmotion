% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/transformer_scores.R
\name{transformer_scores}
\alias{transformer_scores}
\title{Sentiment Analysis Scores}
\usage{
transformer_scores(
  text,
  classes,
  multiple_classes = FALSE,
  transformer = c("cross-encoder-distilroberta", "facebook-bart"),
  preprocess = TRUE,
  path_to_python = NULL,
  keep_in_env = TRUE,
  envir = 1
)
}
\arguments{
\item{text}{Character vector or list.
Text in a vector or list data format}

\item{classes}{Character vector.
Classes to score the text}

\item{multiple_classes}{Boolean.
Whether the text can belong to multiple true classes.
Defaults to \code{FALSE}.
Set to \code{TRUE} to get scores with multiple classes}

\item{transformer}{Character.
Specific zero-shot sentiment analysis transformer
to be used. Default options:

\itemize{

\item{\code{"cross-encoder-distilroberta"}}
{Uses \href{https://huggingface.co/cross-encoder/nli-distilroberta-base}{Cross-Encoder's Natural Language Interface DistilRoBERTa Base}
zero-shot classification model trained on the
\href{https://nlp.stanford.edu/projects/snli/}{Stanford Natural Language Inference}
(SNLI) corpus and 
\href{https://huggingface.co/datasets/multi_nli}{MultiNLI} datasets}

\item{\code{"facebook-bart"}}
{Uses \href{https://huggingface.co/facebook/bart-large-mnli}{Facebook's BART Large}
zero-shot classification model trained on the
\href{https://huggingface.co/datasets/multi_nli}{Multi-Genre Natural Language
Inference} (MultiNLI) dataset}

}

Defaults to \code{"cross-encoder-distilroberta"}

Also allows any zero-shot classification models with a pipeline
from \href{https://huggingface.co/models?pipeline_tag=zero-shot-classification}{huggingface}
to be used by using the specified name (e.g., \code{"typeform/distilbert-base-uncased-mnli"}; see Examples)}

\item{preprocess}{Boolean.
Should basic preprocessing be applied?
Includes making lowercase, keeping only alphanumeric characters,
removing escape characters, removing repeated characters,
and removing white space.
Defaults to \code{TRUE}.
Transformers generally are OK without preprocessing and handle
many of these functions internally, so setting to \code{FALSE}
will not change performance much}

\item{path_to_python}{Character.
Path to specify where "python.exe" is located on your computer.
Defaults to \code{NULL}, which will use \code{\link[reticulate]{py_available}}
to find available Python or Anaconda}

\item{keep_in_env}{Boolean.
Whether the classifier should be kept in your global environment.
Defaults to \code{TRUE}.
By keeping the classifier in your environment, you can skip
re-loading the classifier every time you run this function.
\code{TRUE} is recommended}

\item{envir}{Numeric.
Environment for the classifier to be saved for repeated use.
Defaults to the global environment}
}
\value{
Returns probabilities for the text classes
}
\description{
Uses sentiment analysis pipelines from \href{https://huggingface.co}{huggingface}
to compute probabilities that the text corresponds to the specified classes
}
\details{
This function requires that you have both Python and the 
"transformers" module installed on your computer. For help installing Python 
and Python modules, see \code{browseVignettes("transforEmotion")} and click on
the "Python Setup" vignette.

Once both Python and the "transformers" module are installed, the
function will automatically download the necessary model to compute the
scores.
}
\examples{
# Load data
data(neo_ipip_extraversion)

# Example text 
text <- neo_ipip_extraversion$friendliness[1:5]

\dontrun{
# Cross-Encoder DistilRoBERTa
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 )
)

# Facebook BART Large
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 transformer = "facebook-bart"
)

# Directly from huggingface: typeform/distilbert-base-uncased-mnli
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 transformer = "typeform/distilbert-base-uncased-mnli"
)
}

}
\references{
# BART
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ... & Zettlemoyer, L. (2019).
Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.
\emph{arXiv preprint arXiv:1910.13461}.

# RoBERTa
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019).
Roberta: A robustly optimized bert pretraining approach.
\emph{arXiv preprint arXiv:1907.11692}.

Yin, W., Hay, J., & Roth, D. (2019).
Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach.
\emph{arXiv preprint arXiv:1909.00161}.
}
\author{
Alexander P. Christensen <alexpaulchristensen@gmail.com>
}
