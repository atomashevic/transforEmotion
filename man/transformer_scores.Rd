% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/transformer_scores.R
\name{transformer_scores}
\alias{transformer_scores}
\title{Sentiment Analysis Scores}
\usage{
transformer_scores(
  text,
  classes,
  multiple_classes = FALSE,
  transformer = c("cross-encoder-roberta", "cross-encoder-distilroberta",
    "facebook-bart"),
  device = c("auto", "cpu", "cuda"),
  preprocess = FALSE,
  keep_in_env = TRUE,
  envir = 1,
  local_model_path = NULL
)
}
\arguments{
\item{text}{Character vector or list.
Text in a vector or list data format}

\item{classes}{Character vector.
Classes to score the text}

\item{multiple_classes}{Boolean.
Whether the text can belong to multiple true classes.
Defaults to \code{FALSE}.
Set to \code{TRUE} to get scores with multiple classes}

\item{transformer}{Character.
Specific zero-shot sentiment analysis transformer
to be used. Default options:

\describe{

\item{\code{"cross-encoder-roberta"}}{Uses \href{https://huggingface.co/cross-encoder/nli-roberta-base}{Cross-Encoder's Natural Language Interface RoBERTa Base}
zero-shot classification model trained on the
\href{https://nlp.stanford.edu/projects/snli/}{Stanford Natural Language Inference}
(SNLI) corpus and
\href{https://huggingface.co/datasets/multi_nli}{MultiNLI} datasets}

\item{\code{"cross-encoder-distilroberta"}}{Uses \href{https://huggingface.co/cross-encoder/nli-distilroberta-base}{Cross-Encoder's Natural Language Interface DistilRoBERTa Base}
zero-shot classification model trained on the
\href{https://nlp.stanford.edu/projects/snli/}{Stanford Natural Language Inference}
(SNLI) corpus and
\href{https://huggingface.co/datasets/multi_nli}{MultiNLI} datasets. The DistilRoBERTa
is intended to be a smaller, more lightweight version of \code{"cross-encoder-roberta"},
that sacrifices some accuracy for much faster speed (see
\href{https://www.sbert.net/docs/cross_encoder/pretrained_models.html#nli}{https://www.sbert.net/docs/cross_encoder/pretrained_models.html#nli})}

\item{\code{"facebook-bart"}}{Uses \href{https://huggingface.co/facebook/bart-large-mnli}{Facebook's BART Large}
zero-shot classification model trained on the
\href{https://huggingface.co/datasets/multi_nli}{Multi-Genre Natural Language
Inference} (MultiNLI) dataset}

}

Defaults to \code{"cross-encoder-distilroberta"}

Also allows any zero-shot classification models with a pipeline
from \href{https://huggingface.co/models?pipeline_tag=zero-shot-classification}{huggingface}
to be used by using the specified name (e.g., \code{"typeform/distilbert-base-uncased-mnli"}; see Examples)

Note: Using custom HuggingFace model IDs beyond the recommended models is done at your own risk.
Large models may cause memory issues or crashes, especially on systems with limited resources.
The package has been optimized and tested with the recommended models listed above.}

\item{device}{Character.
Whether to use CPU or GPU for inference.
Defaults to \code{"auto"} which will use
GPU over CPU (if CUDA-capable GPU is setup).
Set to \code{"cpu"} to perform over CPU}

\item{preprocess}{Boolean.
Should basic preprocessing be applied?
Includes making lowercase, keeping only alphanumeric characters,
removing escape characters, removing repeated characters,
and removing white space.
Defaults to \code{FALSE}.
Transformers generally are OK without preprocessing and handle
many of these functions internally, so setting to \code{TRUE}
will not change performance much}

\item{keep_in_env}{Boolean.
Whether the classifier should be kept in your global environment.
Defaults to \code{TRUE}.
By keeping the classifier in your environment, you can skip
re-loading the classifier every time you run this function.
\code{TRUE} is recommended}

\item{envir}{Numeric.
Environment for the classifier to be saved for repeated use.
Defaults to the global environment}

\item{local_model_path}{Optional. Path to a local directory containing a pre-downloaded
  HuggingFace model. If provided, the model will be loaded from this directory instead
  of being downloaded from HuggingFace. This is useful for offline usage or for using
  custom fine-tuned models.

  On Linux/Mac, look in ~/.cache/huggingface/hub/ folder for downloaded models.
  Navigate to the snapshots folder for the relevant model and point to the directory
  which contains the config.json file. For example:
  "/home/username/.cache/huggingface/hub/models--cross-encoder--nli-distilroberta-base/snapshots/b5b020e8117e1ddc6a0c7ed0fd22c0e679edf0fa/"

  On Windows, the base path is C:\\Users\\USERNAME\\.cache\\huggingface\\transformers\\

  Warning: Using very large models from local paths may cause memory issues or crashes
  depending on your system's resources.}
}
\value{
Returns probabilities for the text classes
}
\description{
Uses sentiment analysis pipelines from \href{https://huggingface.co}{huggingface}
to compute probabilities that the text corresponds to the specified classes
}
\section{Data Privacy}{

  All processing is done locally with the downloaded model,
  and your text is never sent to any remote server or third-party.
}

\examples{
# Load data
data(neo_ipip_extraversion)

# Example text
text <- neo_ipip_extraversion$friendliness[1:5]

\dontrun{
# Cross-Encoder DistilRoBERTa
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 )
)

# Facebook BART Large
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 transformer = "facebook-bart"
)

# Directly from huggingface: typeform/distilbert-base-uncased-mnli
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 transformer = "typeform/distilbert-base-uncased-mnli"
)
}

}
\references{
# BART \cr
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ... & Zettlemoyer, L. (2019).
Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.
\emph{arXiv preprint arXiv:1910.13461}.

# RoBERTa \cr
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019).
Roberta: A robustly optimized bert pretraining approach.
\emph{arXiv preprint arXiv:1907.11692}.

# Zero-shot classification \cr
Yin, W., Hay, J., & Roth, D. (2019).
Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach.
\emph{arXiv preprint arXiv:1909.00161}.

# MultiNLI dataset \cr
Williams, A., Nangia, N., & Bowman, S. R. (2017).
A broad-coverage challenge corpus for sentence understanding through inference.
\emph{arXiv preprint arXiv:1704.05426}.
}
\author{
Alexander P. Christensen <alexpaulchristensen@gmail.com>
}
